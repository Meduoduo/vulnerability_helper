'''
    NOTICE: this module is just for update latest exploitdb data, not for kasumi spider

    exploitdb spider is the module that take responsibility to download exploitdb data
    but only download to cache, dose not offer any interface to LLM
'''

from logger import Logger
import os
import os.path
import wget
import zipfile
import shutil
import pandas as pd
import datetime
from pandas import DataFrame
from typing import Iterator, Dict
from config import ARCHIEVE_URL, ARCHIEVE_PATH, ARCHIEVE_FILE, ARCHIEVE_DIR
from model import Vulnerability

exploitdb_df: DataFrame = None

class ExploitDB():
    def load(self) -> None:
        try:
            # check if exploit-db has been downloaded
            if not os.path.exists(ARCHIEVE_PATH) or not os.path.exists(os.path.join(ARCHIEVE_PATH, ARCHIEVE_FILE)):
                # download exploit-db
                Logger.info('Downloading exploit-db...')
                # create cache folder
                os.makedirs(ARCHIEVE_PATH, exist_ok=True)
                # download exploit-db
                wget.download(ARCHIEVE_URL, os.path.join(ARCHIEVE_PATH, ARCHIEVE_FILE))
                Logger.info('Downloaded exploit-db')
            else:
                Logger.info('Exploit-db has been downloaded')
        except:
            Logger.error('Failed to download exploit-db')
            raise

        try:
            # check if exploit-db has been extracted
            if not os.path.exists(os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR)):
                # extract exploit-db
                Logger.info('Extracting exploit-db...')
                # extract exploit-db
                with zipfile.ZipFile(os.path.join(ARCHIEVE_PATH, ARCHIEVE_FILE), 'r') as zip_ref:
                    zip_ref.extractall(os.path.join(ARCHIEVE_PATH, "temp"))
                
                Logger.info('Extracted exploit-db, moving files...')
                Logger.info('It may take a long time, please wait patiently or just copy the files in %s to %s' % (os.path.join(ARCHIEVE_PATH, "temp"), os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR)))
                # get directory name in the root directory
                root_dir = os.listdir(os.path.join(ARCHIEVE_PATH, "temp"))[0]
                # move files to root directory
                shutil.move(os.path.join(ARCHIEVE_PATH, "temp", root_dir), os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR))
                Logger.info('Extracted exploit-db')
            else:
                Logger.info('Exploit-db has been extracted')
        except:
            Logger.error('Failed to extract exploit-db')
            raise

        Logger.info(f'exploit-db has been saved in {os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR)}')
        Logger.info(f'to detect new vulnerabilities, please create {os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR, "exploitdb_new.csv")}')

    def new(self) -> Iterator[Vulnerability]:
        # check if exploitdb_new.csv exists
        if not os.path.exists(os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR, 'exploitdb_new.csv')):
            return []
        # read from csv
        exploitdb_df = pd.read_csv(os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR, 'exploitdb_new.csv'))
        # iterate
        for index, row in exploitdb_df.iterrows():
            Logger.info('Processing exploit-db : %d/%d' % (index, len(exploitdb_df)))
            yield self.load_exploitdb_data(row.to_dict())
        # delete csv
        os.remove(os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR, 'exploitdb_new.csv'))

    def load_exploitdb_data(self, data: Dict) -> Vulnerability:
        vul = Vulnerability()
        vul.set_source('exploitdb')
        
        exp = ''

        # read data
        try:
            with open(os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR, data['file']), 'r') as f:
                exp = f.read()
        except:
            Logger.warning('Cannot read file: %s' % os.path.join(ARCHIEVE_PATH, ARCHIEVE_DIR, data['file']))

        if not exp or exp == '':
            exp = 'No exploit data'

        # set data
        vul.set_exp(exp=exp)

        description = data.get('description', 'No description')
        vul.set_description(description=description)

        publish_date = data.get('date_published', 'No publish date')

        # parse publish date
        try:
            publish_date = datetime.datetime.strptime(publish_date, '%Y-%m-%d')
        except:
            publish_date = datetime.datetime.now()

        vul.set_date(date=publish_date)

        author = data.get('author', 'No author')
        vul.set_authors([author])

        platform = data.get('platform', 'No platform')
        vul.set_platforms([{
            'platform': platform,
            'version': ['all']
        }])

        codes = data.get('codes', '')
        # parse codes to string if codes is number
        if isinstance(codes, int) or isinstance(codes, float):
            codes = str(codes)
        if codes == 'nan':
            codes = ''
        codes = codes.split(';')
        codes = list(filter(lambda x: x.strip() != '', codes))
        vul.set_codes(codes=codes)

        names = data.get('aliases', '')
        # parse names to string if names is number
        if isinstance(names, int) or isinstance(names, float):
            names = str(names)
        if names == 'nan':
            names = ''
        names = names.split(';')
        names = list(filter(lambda x: x.strip() != '', names))
        vul.set_names(names=names)

        # set references
        references = data.get('source_url', '')
        if references == '' or references is None or not references:
            references = []
        else:
            references = [references]
        vul.set_references(references=references)
        vul.set_original(original=data)

        return vul